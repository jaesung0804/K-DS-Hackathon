#%%
"""
Reference:
[1] https://www.linkedin.com/pulse/question-answer-bot-using-openai-langchain-faiss-satish-srinivasan/
"""
#%%
import os
import json
import re
import torch
import pandas as pd
import numpy as np
import datetime
from tqdm.notebook import tqdm
tqdm.pandas()
from module.embedding import get_embedding
import streamlit as st
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import openai
from openai import OpenAI
from transformers import AutoTokenizer, AutoModel
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib import font_manager, rc
import plotly.graph_objs as go
import plotly.express as px
from tenacity import retry, wait_random_exponential, stop_after_attempt
import FinanceDataReader as fdr
import datetime
import platform
#%%
def main():
    # https://huggingface.co/jhgan/ko-sroberta-multitask
    tokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sroberta-multitask')
    model = AutoModel.from_pretrained('jhgan/ko-sroberta-multitask')
    
    directory = "./log"
    if not os.path.exists(directory):
        os.makedirs(directory)
    
    csv_file = "./log/input_log.csv"
    if os.path.exists(csv_file):
        input_log = pd.read_csv(csv_file, index_col=False)
    else:
        input_log = pd.DataFrame(
            {'time': np.array([]).astype('datetime64[ns]'),
            'query': np.array([]),
            'sector': np.array([]),
            'company': np.array([]),
            'start date': np.array([]),
            'end date': np.array([]),
            'topk': np.array([]),
            'feedback': np.array([]).astype(int),
            }
        )

    keywords_df = pd.read_csv('./data/trend_data/kewords.csv',encoding='utf-8-sig')
    st.header("""
    í‚¤ì›Œë“œ ê¸°ë°˜ì˜ ì£¼ìš” ë¬¸ì¥ ì¶”ì¶œì„ í†µí•œ ìš”ì•½ ì‹œìŠ¤í…œ
    #### ì„œìš¸ì‹œë¦½ëŒ€í•™êµ í†µê³„ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤í•™ê³¼ íŒ€ VANILLA ğŸ¦
    """)
    sectors = ['ë°˜ë„ì²´', 'ë°”ì´ì˜¤', '2ì°¨ì „ì§€', 'í†µì‹ ì‚¬', 'ìë™ì°¨', 'IT', 'ì§€ìˆ˜']
    
    st.markdown("""---""")
    selected_sector = st.selectbox('##### ğŸ“ˆ í…Œë§ˆë¥¼ ì„ íƒí•˜ì„¸ìš”', sectors)
    companys = {'ë°˜ë„ì²´':['ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤'],
                'ë°”ì´ì˜¤':['ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'ì…€íŠ¸ë¦¬ì˜¨'],
                '2ì°¨ì „ì§€':['ì‚¼ì„±SDI', 'LGí™”í•™'],
                'í†µì‹ ì‚¬':['KT', 'SKí…”ë ˆì½¤', "LGìœ í”ŒëŸ¬ìŠ¤"],
                'ìë™ì°¨':['í˜„ëŒ€ì°¨', 'ê¸°ì•„'],
                'IT':['NAVER', 'ì¹´ì¹´ì˜¤'],
                'ì§€ìˆ˜':['ì½”ìŠ¤í”¼', 'ë‹ˆì¼€ì´ì§€ìˆ˜', 'ë‚˜ìŠ¤ë‹¥', 'ìƒí•´ì¢…í•©ì§€ìˆ˜']}
    
    selected_company = st.selectbox('##### ğŸ“ˆ ì¢…ëª©ì„ ì„ íƒí•˜ì„¸ìš”', companys[selected_sector])
    
    company_dict = {
        'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤': 'ì‚¼ì„±ë°”ì´ì˜¤',
        'SKí•˜ì´ë‹‰ìŠ¤': 'skí•˜ì´ë‹‰ìŠ¤',
        'ì‚¼ì„±SDI': 'ì‚¼ì„±sdi',
        'LGí™”í•™': 'lgí™”í•™',
        'NAVER': 'ë„¤ì´ë²„',
        'SKí…”ë ˆì½¤': 'SKT',
        'ë‹ˆì¼€ì´ì§€ìˆ˜': 'ë‹›ì¼€ì´ì§€ìˆ˜',
    }
    if selected_company in company_dict.keys():
       selected_company = company_dict.get(selected_company)
    keywords_list = keywords_df[selected_company].values.tolist()

    start_date = st.date_input("##### ğŸ—“ï¸ ì‹œì‘ ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”", value=datetime.date(2018, 11, 2))
    end_date = st.date_input("##### ğŸ—“ï¸ ì¢…ë£Œ ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”", value=datetime.date(2023, 10, 31))
    start_date = datetime.datetime.combine(start_date, datetime.datetime.min.time())
    end_date = datetime.datetime.combine(end_date, datetime.datetime.min.time())

    # test
    # selected_sector = 'ë°”ì´ì˜¤'
    # selected_company = 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤'
    # query = 'ë°˜ë„ì²´'
    # start_date = datetime.datetime(2021,12,2)
    # end_date = datetime.datetime(2023,10,31)
    
    df = pd.read_csv(f"./data/trend_data/trend_news_{selected_company}.csv")
    df['Date'] = pd.to_datetime(df['Date'])
    df_new = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]
    start_idx = df_new.index[0]
    end_idx = df_new.index[-1]
    df_new = df_new.reset_index(drop=True)

    flag = df_new["news"].isna().sum()
    if flag > 0:
        drop_idx = list(df_new[df_new["news"].isna()].index)
        df_new = df_new.drop(index=drop_idx)
    
    file_path = f"./data/trend_data/trend_embedd_database_{selected_company}.pt"
    embeddings = torch.load(file_path)[start_idx:end_idx+1]
    
    if flag > 0:
        embeddings = [e for j, e in enumerate(embeddings) if j not in drop_idx]
    
    assert len(embeddings) == len(df_new)
    
    df_new["news"] = df_new["news"].apply(lambda x: re.sub("fn", "", x))
    
    topk = st.selectbox(
        '##### í‚¤ì›Œë“œë¥¼ ì´ìš©í•´ ì¶”ì¶œí•  ë‰´ìŠ¤ ê¸°ì‚¬ í—¤ë“œë¼ì¸ì˜ ê°œìˆ˜', [5, 10, 15, 20, 25, 30], index=1)
    
    # ì„ íƒëœ ì˜µì…˜ ì¶œë ¥
    # st.write('- ì„ íƒí•œ ì¢…ëª©:', selected_company)
    # st.write('- ì¶”ì²œ í‚¤ì›Œë“œ:', ', '.join(keywords_list[:10]))
    st.write('##### ì¶”ì²œ í‚¤ì›Œë“œ: ' + ' '.join([f"`{x.upper()}`" for x in keywords_list[:10]]))
    
    with st.form('input_form'):
        query = st.text_input(
            "",
            # """
            # ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³ , [Enter]ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.
            # """,
            label_visibility='collapsed').lower()
        st.form_submit_button('ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³ , [Enter]ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.')
        # col1, col2 = st.columns(2)
        # with col1:
        #     st.form_submit_button('ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³ , [Enter]ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.')
        # with col2:
        #     query = st.text_input(
        #         "",
        #         # """
        #         # ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³ , [Enter]ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.
        #         # """,
        #         label_visibility='collapsed')

    if query:
        
        if 'summary' not in st.session_state:
            st.session_state.summary = ""
        
        tab1, tab2, tab3 = st.tabs(["Top-K ë‰´ìŠ¤ í—¤ë“œë¼ì¸", "ì‹œê°í™”", "ChatGPT ë³´ì • ê²°ê³¼"])
        
        keywords = query
        query_emb = get_embedding(keywords, tokenizer, model)

        score = []
        for i in range(len(df_new)):
            score.append((i, cosine_similarity(embeddings[i], query_emb)[0][0]))

        # ranking
        top_idx = [t[0] for t in sorted(score, key=lambda x: x[1], reverse=True)[:50]] # default
        top_idx = sorted(top_idx)
        
        keywords = selected_company + ' ' + query
        query_emb = get_embedding(keywords, tokenizer, model)
        
        score = []
        for i in top_idx:
            score.append((i, cosine_similarity(embeddings[i], query_emb)[0][0]))
        
        # re-ranking
        top_idx2 = [t[0] for t in sorted(score, key=lambda x: x[1], reverse=True)[:topk]]
        top_idx2 = sorted(top_idx2)
        
        output = ""
        df_new['Date'] = df_new['Date'].astype(str)            
        for i in top_idx2:
            output += "##### [" + df_new['Date'].iloc[i] + "] "
            output += df_new['news'].iloc[i]
            output += " " + f"(ì¤‘ìš”ë„: {abs(df_new['Predicted_Without'].iloc[i]): .3f})" + "\n\n"
                    
        # df['news'].iloc[bottom_idx]
        if len(output) == 0 : output = 'ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'
        with tab1:
            st.write("### ì›ë¬¸ì—ì„œ ì¶”ì¶œëœ ì¤‘ìš” ë¬¸ì¥")
            st.write(output)

        vis_data = {
            'Date': [],
            'News': [],
            'Predicted_Without': []
        } 
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ vis_dataì— ì¶”ê°€í•©ë‹ˆë‹¤.
        for i in top_idx2:
            vis_data['Date'].append(df_new['Date'].iloc[i])
            vis_data['News'].append(df_new['news'].iloc[i])
            vis_data['Predicted_Without'].append(df_new['Predicted_Without'].iloc[i])

        
        if platform.system() == 'Windows':
            rc('font', family='Malgun Gothic')
        elif platform.system() == 'Darwin': # Mac
            rc('font', family='AppleGothic')
        else: #linux
            rc('font', family='NanumGothic')
        # font_path = 'C:\Windows\Fonts\MALGUNSL.ttf' 
        # font_name = font_manager.FontProperties(fname=font_path).get_name()
        # rc('font', family=font_name)
                    
        vis_df = pd.DataFrame(vis_data)
        vis_df['Date'] = pd.to_datetime(vis_df['Date'])

        # st.markdown("""---""")
        with tab2:
            st.write("### [ê·¸ë¦¼] í‚¤ì›Œë“œì™€ ê´€ë ¨ì„±ì´ ë†’ì€ ë‰´ìŠ¤ í—¤ë“œë¼ì¸")
            
            # Plotly ê·¸ë˜í”„ ìƒì„±
            fig = go.Figure()
            
            y_max = max([abs(x) + 0.5 for x in vis_df['Predicted_Without']])

            # ì ë“¤ì„ ì¶”ê°€
            for index, row in vis_df.iterrows():
                fig.add_trace(go.Scatter(
                    x=[row['Date']],  # ë‚ ì§œëŠ” ì—¬ê¸°ì„œ ë¬¸ìì—´ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
                    y=[abs(row['Predicted_Without'])],
                    mode='markers',
                    marker=dict(size=12),
                    customdata=[[  # customdataëŠ” ê° ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•œ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
                        row['Date'].strftime('%Y-%m-%d'),  # ë‚ ì§œëŠ” ë¬¸ìì—´ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
                        row['News'],
                        abs(row['Predicted_Without'])
                    ]],
                    hovertemplate=(
                        "<b>ë‚ ì§œ:</b> %{customdata[0]}<br>" +
                        "<b>í—¤ë“œë¼ì¸:</b> %{customdata[1]}<br>" +
                        "<b>ì¤‘ìš”ë„:</b> %{customdata[2]:.3f}<extra></extra>"  # <extra></extra>ëŠ” ë¶ˆí•„ìš”í•œ trace ì´ë¦„ì„ ìˆ¨ê¹ë‹ˆë‹¤.
                    ),
                    showlegend=False  # ì´ ì¤„ì„ ì¶”ê°€í•˜ì—¬ ê° traceì˜ ë²”ë¡€ í‘œì‹œë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
                ))

            # ë ˆì´ì•„ì›ƒ ì„¤ì •
            fig.update_traces(
                hoverlabel=dict(
                    bgcolor="white",
                    font_size=18,  # í°íŠ¸ í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
                    # font_family=font_name,
                    font_color="black"
                )
            )

            fig.update_layout(
                title=dict(
                    # text="[ê·¸ë¦¼] í‚¤ì›Œë“œì™€ ê´€ë ¨ì„±ì´ ë†’ì€ ë‰´ìŠ¤ í—¤ë“œë¼ì¸", 
                    text="",
                    font=dict(size=24), 
                    automargin=True),
                xaxis=dict(
                    # title='ë‚ ì§œ',
                    # showgrid=True,
                    # gridwidth=1,
                    # gridcolor='LightPink',
                    # griddash='dot',
                    tickfont = dict(size=18)
                ),
                yaxis=dict(
                    title='ì¤‘ìš”ë„',
                    zeroline=True,
                    zerolinewidth=1,  # Zero lineì˜ ë‘ê»˜
                    zerolinecolor='LightPink',  # Zero lineì˜ ìƒ‰ìƒì„ ë³€ê²½í•©ë‹ˆë‹¤.
                    range=(0, y_max),
                    showgrid=True,
                    gridwidth=1,
                    gridcolor='LightPink',
                    griddash='dot',
                    tickfont = dict(size=18)
                ),
                hovermode='closest',
                showlegend=False  # ì´ ì¤„ì„ ì¶”ê°€í•˜ì—¬ ì „ì²´ ê·¸ë˜í”„ì˜ ë²”ë¡€ë¥¼ ìˆ¨ê¹ë‹ˆë‹¤.
            )

            # Streamlitì— ê·¸ë˜í”„ë¥¼ í‘œì‹œ
            st.plotly_chart(fig, use_container_width=True)

        # st.markdown("""---""")
        with tab3:
            st.write("### ChatGPTë¥¼ ì´ìš©í•œ ê²°ê³¼ ë³´ì •")

            # ìš”ì•½ë¬¸ì„ í‘œì‹œí•  ì˜ì—­
            summary_placeholder = st.empty()

            vis_df['Date'] = vis_df['Date'].astype(str)
            
            if st.button('ìš”ì•½í•˜ê¸°'):

                # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
                            
                # ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìš”ì•½ ìš”ì²­ ë¬¸ìì—´ ìƒì„±
                news_for_summary = "\n".join(vis_df['Date'] + ": " + vis_df['News'])
                prompt = """
                Please generate an appropriate response to the given requirements and instructions.\n
                Requirements and Instructions : \n
                1. You must transform news headlines in the form of several paragraphs in korean. \n
                2. Only one year should be included in one paragraph. 
                3. Everything about each year should be included in the corresponding paragraph.
                    For example, your response should be in the form of "In 2019, there was an event called a, and there was also an event called b." \n
                4. All given news headlines for a year must be included in the corresponding paragraph. \n
                5. All news headlines should be included in your reponse.
                6. Facts must not be changed. \n
                7. Don't mention a specific date, just mention the year. \n
                """+ "\nData and News headlines : \n" + news_for_summary

                @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(100))
                def completions_with_backoff(**kwargs):
                    return client.chat.completions.create(**kwargs)
                # @backoff.on_exception(backoff.expo, openai.error.RateLimitError)
                # def completions_with_backoff(**kwargs):
                #     return openai.ChatCompletion.create(**kwargs)
                response = completions_with_backoff(
                    model="gpt-3.5-turbo-1106", 
                    messages=[
                        {
                            "role": "system",
                            "content": "You're a helpful assistant that transforms news headlines in the form of several paragraphs in korean.",
                        },
                        {
                            "role": "user",
                            "content": prompt,
                        }
                    ],
                    temperature=0.5)
                
                # ìš”ì•½ ê²°ê³¼ í‘œì‹œ
                summary = response.choices[0].message.content.strip()
                st.session_state.summary = summary
            # st.write(summary)
            summary_placeholder.text_area("", st.session_state.summary, height=400)
                
    else:
        st.write('ê²€ìƒ‰ëœ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.')
    
    st.markdown("""---""")
    st.write(
    """
    ìš”ì•½ ê²°ê³¼ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”. ì—¬ëŸ¬ë¶„ì˜ ì†Œì¤‘í•œ ì˜ê²¬ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
    """
    )
    
    col1, col2, col3 = st.columns([0.15, 0.15, 0.7])
    with col1:
        like_button = st.button("ì¢‹ì•„ìš” ğŸ‘")
    with col2:
        dislike_button = st.button("ì‹«ì–´ìš” ğŸ‘")
    with col3:
        st.empty()
         
    if like_button:
        timestamp = datetime.datetime.now()
        input_log.loc[len(input_log)] = [
            timestamp, query, selected_sector, selected_company, start_date, end_date, topk, 1] # like
        input_log.to_csv(csv_file, index=False)
    elif dislike_button:
        timestamp = datetime.datetime.now()
        input_log.loc[len(input_log)] = [
            timestamp, query, selected_sector, selected_company, start_date, end_date, topk, 0] # dislike
        input_log.to_csv(csv_file, index=False)
    # else:
    #     st.markdown("*ë¡œê·¸ ê¸°ë¡ì„ ìœ„í•´ ì œì¶œì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")

    st.markdown("""---""")
    st.write("###### MADE BY ì•ˆìŠ¹í™˜, ë°•ê¸°ì •, ë°•ì¬ì„±, ìš°ê²½ë™, ì„ì¬ì„±")

#%%
if __name__ == "__main__":
    main()


# please write above headlines in the form of several paragraphs in korean.
